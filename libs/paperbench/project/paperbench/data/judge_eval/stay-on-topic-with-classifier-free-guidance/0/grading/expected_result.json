{
    "id": "6fa267fe-5014-4fc2-b49f-2758b0cf3545",
    "requirements": "Reproduce the paper 'Stay on topic with Classifier-Free Guidance'",
    "weight": 1,
    "score": 0.48152958152958153,
    "valid_score": true,
    "task_category": null,
    "explanation": "Aggregated score from sub-tasks.",
    "judge_metadata": null,
    "sub_tasks": [
        {
            "id": "9aee214f-db29-4b6e-9b8b-c9bca32f266c",
            "requirements": "CFG is shown to cause a nontrivial and consistent boost in performance across the suite of zero-shot benchmarks, with the exception of ARC (challenge) and Winogrande. Results follow Section 3.1 and Table 5.",
            "weight": 4,
            "score": 0.9047619047619048,
            "valid_score": true,
            "task_category": null,
            "explanation": "Aggregated score from sub-tasks.",
            "judge_metadata": null,
            "sub_tasks": [
                {
                    "id": "19d30608-f6dc-40ce-b57d-4e4374035035",
                    "requirements": "The zero-shot benchmark datasets (ARC-c, ARC-e, BoolQ, HellaSwag, PiQA, SciQ, TriviaQA, WinoGrande, Lambada) have been acquired as described in Section 3.1.",
                    "weight": 1,
                    "score": 1.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "8174064a-24ca-4094-a040-25d145f2e764",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset ARC-c (https://huggingface.co/datasets/allenai/ai2_arc) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file calls harness.evaluate on TASKS_SHORT which includes ARC-c. Harness.evaluate relies on the eleuther harness which points to the right version of ARC-c",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "43b57f14-20f5-494d-a275-d1e52303249a",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset ARC-e (https://huggingface.co/datasets/allenai/ai2_arc) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file calls harness.evaluate on TASKS_SHORT which includes ARC-e. Harness.evaluate relies on the eleuther harness which points to the right version of ARC-e",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "79ab826f-06c7-43eb-ad3c-ad17fada9878",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset BoolQ (https://huggingface.co/datasets/google/boolq) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file calls harness.evaluate on TASKS_SHORT which includes BoolQ. Harness.evaluate relies on the eleuther harness which points to the right version of BoolQ",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "a215688a-0d56-42ce-bc96-61c395391a53",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset HellaSwag (https://rowanzellers.com/hellaswag/) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file can be called harness.evaluate on TASKS which includes HellaSwag, by omitting the --short flag. Harness.evaluate relies on the eleuther harness which points to the right version of HellaSwag",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "dbed549d-88ee-4990-a8f6-c628dd1641c2",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset PiQA (https://github.com/ybisk/ybisk.github.io/tree/master/piqa) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file can be called harness.evaluate on TASKS which includes PiQA, by omitting the --short flag. Harness.evaluate relies on the eleuther harness which points to the right version of PiQA",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "873a2ed0-4c28-4548-9f82-099b29b3a8d8",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset SciQ (https://huggingface.co/datasets/allenai/sciq) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file calls harness.evaluate on TASKS_SHORT which includes SciQ. Harness.evaluate relies on the eleuther harness which points to the right version of SciQ",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "2213f8f0-f2a6-4fae-a61d-4805c3720969",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset TriviaQA (https://huggingface.co/datasets/mandarjoshi/trivia_qa) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file can be called harness.evaluate on TASKS which includes TriviaQA, by omitting the --short flag. Harness.evaluate relies on the eleuther harness which points to the right version of TriviaQA",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "b78526fe-3d10-4b8c-a9c5-d139f41bd292",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset WinoGrande (https://huggingface.co/datasets/allenai/winogrande) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file calls harness.evaluate on TASKS_SHORT which includes WinoGrande. Harness.evaluate relies on the eleuther harness which points to the right version of WinoGrande",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "8329045d-344f-4d51-b95a-0b0056c4220a",
                            "requirements": "Code for acquiring the zero-shot benchmark dataset Lambada (https://huggingface.co/datasets/EleutherAI/lambada_openai) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.1.py file calls harness.evaluate on TASKS_SHORT which includes Lambada (OpenAI). Harness.evaluate relies on the eleuther harness which points to the right version of Lambada (OpenAI)",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "837b7902-ec4e-4aa4-b17c-bae83104c462",
                    "requirements": "The evaluation setup has been reproduced, either by directly using the Language Model Evaluation Harness (Gao et al., 2021, github.com/EleutherAI/lm-evaluation-harness) or by reproducing its setup",
                    "weight": 1,
                    "score": 1.0,
                    "valid_score": true,
                    "task_category": "Code Development",
                    "explanation": "run_3.1.py uses the evaluate from harness, which uses the eleuther LM eval harness.",
                    "judge_metadata": null,
                    "sub_tasks": []
                },
                {
                    "id": "d86cb9e3-d8d0-4013-96a6-9f982eefd4f7",
                    "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on GPT2 and Pythia models",
                    "weight": 2,
                    "score": 1.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "a012098d-2cd3-4c39-9613-148f46e67482",
                            "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on the GPT2 family of models",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "6103dd0e-8fd8-4b5f-b4f2-88c553815a15",
                                    "requirements": "Classifier-Free Guidance (CFG) inference is implemented as described in equation 7 in Section 2.2 such that it can be run on the GPT2 family of models",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "The classifier_free_guidance fn in cfg.py implements CFG as outlined in section 2.2, L89 is equivalent to equation 7. GPT2 can be run with CFG via the CFGLogitsProcessor in cfg.py or via the CFGLM in harness.py.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "bc1168ce-1669-4aa8-9f2c-d806a8941d27",
                                    "requirements": "The implementation of CFG which can be used for the GPT2 family of models includes a mutable (not hardcoded) hyper-parameter which can be used to specify the guidance strength",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "Both CFGLM and CFGLogitsProcessor call classifier_free_guidance and all employ a corresponding guidance_scale hparam",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "bcec9a7d-70da-45a0-a588-abba95fa574b",
                                    "requirements": "As outlined in Section 3.1, the implementation of CFG which can be used for the GPT2 family of models starts the unconditional prompt $\\log p_{\\theta}(w_i | w_{j < i})$ with the last token of the initial prompt for the GPT2 family of models",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "This is implemented in L69-72 of cfg.py",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                }
                            ]
                        },
                        {
                            "id": "f7386fb8-da3c-4cb4-9a53-28447e7677c0",
                            "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on the Pythia family of models",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "89d93abb-87b9-43fc-a58f-e222c879b3a2",
                                    "requirements": "Classifier-Free Guidance (CFG) inference is implemented as described in equation 7 in Section 2.2 such that it can be run on the Pythia family of models",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "The classifier_free_guidance fn in cfg.py implements CFG as outlined in section 2.2, L89 is equivalent to equation 7. Pythia can be run with CFG via the CFGLogitsProcessor in cfg.py or via the CFGLM in harness.py",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "10f12f5c-862c-4f98-bce0-401e0583ca99",
                                    "requirements": "The implementation of CFG which can be used for the Pythia family of models includes a mutable (not hardcoded) hyper-parameter which can be used to specify the guidance strength",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "Both CFGLM and CFGLogitsProcessor call classifier_free_guidance and all employ a corresponding guidance_scale hparam",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "d7afdeb5-d464-4645-bb95-7d5d1dd3088f",
                                    "requirements": "As outlined in Section 3.1, the implementation of CFG which can be used for the Pythia family of models starts the unconditional prompt $\\log p_{\\theta}(w_i | w_{j < i})$ with the last token of the initial prompt for the Pythia family of models",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "This is implemented in L69-72 of cfg.py",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "id": "9252f603-4133-4d84-b588-c82e55468721",
                    "requirements": "The zero-shot benchmark outlined in section 3.1 has been run on the GPT2 and Pythia model families with and without CFG (guidance strength \\gamma=1 and \\gamma=1.5 respectively) for at least 5 of the 9 datasets shown in Table 5. Performance on the benchmark has been noted.",
                    "weight": 2,
                    "score": 0.6666666666666666,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "1a8bfa9c-c656-45a2-ac90-a93be05b505a",
                            "requirements": "The zero-shot benchmark outlined in section 3.1 has been run on the GPT2 model family with and without CFG (guidance strength \\gamma=1 and \\gamma=1.5 respectively) for at least 5 of the 9 datasets shown in Table 5. Performance on the benchmark has been noted.",
                            "weight": 1,
                            "score": 0.6666666666666666,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "5173cc90-71e8-4d36-b056-4c4575037f68",
                                    "requirements": "The zero-shot benchmark outlined in section 3.1 has been run on the GPT2 model family with and without CFG (guidance strength \\gamma=1 and \\gamma=1.5 respectively)",
                                    "weight": 1,
                                    "score": 0.0,
                                    "valid_score": true,
                                    "task_category": "Code Execution",
                                    "explanation": "The zero-shot benchmark has only been run on gpt2 small and gpt medium. So this requirement is not fully satisfied.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "3ca1db72-d16e-4fbf-9747-d1aa24e881bc",
                                    "requirements": "The zero-shot benchmark outlined in section 3.1 has been run on the GPT2 model family for at least 5 of the 9 datasets shown in Table 5.",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Execution",
                                    "explanation": "The zero-shot benchmark has been run on the GPT2 family for 6 out of 9 datasets shown in Table 5, so this requirement is satisfied.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "28101afd-ce96-4e08-b04a-935159ac6f97",
                                    "requirements": "The performance of the evaluated models from the GPT2 model family on the zero-shot benchmark (accuracy or exact match, depending on the dataset) has been measured for at least 5 of the 9 datasets shown in Table 5.",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Execution",
                                    "explanation": "The Eleuther LM evaluation harness handles this and has been run, and the results are saved in results/ for 6 of the 9 datasets, so this requirement is satisfied.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                }
                            ]
                        },
                        {
                            "id": "36a13f7e-a2a1-418e-a7b1-8a16662a1282",
                            "requirements": "The zero-shot benchmark outlined in section 3.1 has been run on the Pythia model family with and without CFG (guidance strength \\gamma=1 and \\gamma=1.5 respectively) for at least 5 of the 9 datasets shown in Table 5. Performance on the benchmark has been noted.",
                            "weight": 1,
                            "score": 0.6666666666666666,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "21799ae3-5194-47ff-8bfb-eda26258335d",
                                    "requirements": "The zero-shot benchmark outlined in section 3.1 has been run on the Pythia model family with and without CFG (guidance strength \\gamma=1 and \\gamma=1.5 respectively)",
                                    "weight": 1,
                                    "score": 0.0,
                                    "valid_score": true,
                                    "task_category": "Code Execution",
                                    "explanation": "The zero-shot benchmark has only been run on pythia 160 and pythia 410. So this requirement is not fully satisfied.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "a01f51d8-42d6-4e91-9a15-0a480f35d0c8",
                                    "requirements": "The zero-shot benchmark outlined in section 3.1 has been run on the Pythia model family for at least 5 of the 9 datasets shown in Table 5.",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Execution",
                                    "explanation": "The zero-shot benchmark has been run on the Pythia family for 6 out of 9 datasets shown in Table 5, so this requirement is satisfied.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "66ed13a5-f926-498c-a6dc-f6a926a76175",
                                    "requirements": "The performance of the evaluated models from the Pythia model family on the zero-shot benchmark (accuracy or exact match, depending on the dataset) has been measured for at least 5 of the 9 datasets shown in Table 5.",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Execution",
                                    "explanation": "The Eleuther LM evaluation harness handles this and has been run, and the results are saved in results/ for 6 of the 9 datasets, so this requirement is satisfied.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "id": "db320ab7-c3bc-4c86-8739-53267fbfd40e",
                    "requirements": "The measured performance metrics on the zero-shot benchmark show that using CFG (\\gamma=1.5) outperforms not using CFG (\\gamma=1) across all evaluated models and datasets, with the exception of ARC (challenge) and Winogrande.",
                    "weight": 1,
                    "score": 1.0,
                    "valid_score": true,
                    "task_category": "Result Analysis",
                    "explanation": "These results are corroborated when observing the tables in results/3.1/, so this requirement is satisfied.",
                    "judge_metadata": null,
                    "sub_tasks": []
                }
            ]
        },
        {
            "id": "9a70e578-aab0-44a4-bc4b-0bb63175b131",
            "requirements": "CFG is shown to increase the % of chains-of-thought ending in a valid answer while increasing model accuracy for small values of gamma. For large values, CFG is shown to reduce accuracy while keeping the invalid % small. Results follow Section 3.2, specifically Figures 2 and 17.",
            "weight": 4,
            "score": 0.5795454545454546,
            "valid_score": true,
            "task_category": null,
            "explanation": "Aggregated score from sub-tasks.",
            "judge_metadata": null,
            "sub_tasks": [
                {
                    "id": "f576bc52-7f07-4fcc-8e27-5b3621ebfbd6",
                    "requirements": "The GSM8K and AQuA datasets for arithmetic reasoning tasks have been acquired, as used in Section 3.2.",
                    "weight": 2,
                    "score": 1.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "135089ea-d617-4538-83cc-29e67a890a93",
                            "requirements": "Code for acquiring the GSM8K dataset for arithmetic reasoning tasks (https://huggingface.co/datasets/openai/gsm8k) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.2.py file calls harness.evaluate on TASKS which includes GSM8K. Harness.evaluate relies on the eleuther harness which points to the right version of GSM8K",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "25ecc1e6-b1a6-472e-8490-c62db7d0943e",
                            "requirements": "Code for acquiring the AQuA datasets for arithmetic reasoning (https://huggingface.co/datasets/nguyen-brat/aqua) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The run_3.2.py file calls harness.evaluate on TASKS which includes AQuA. Harness.evaluate relies on the eleuther harness which points to the right version of AQuA",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "c4d3f07a-0195-40f4-ae06-fa65cf5d7d7a",
                    "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on Guanaco65B and WizardLM-30B models.",
                    "weight": 3,
                    "score": 1.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "311da9f8-f0a5-4531-bbc2-75e8495090f2",
                            "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on the Guanaco65B model.",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "cadfc0e0-58fc-4dc0-9839-c1ddcfecf86f",
                                    "requirements": "Classifier-Free Guidance (CFG) inference is implemented as described in equation 7 in Section 2.2 such that it can be run on the Guanaco65B model.",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "The classifier_free_guidance fn in cfg.py implements CFG as outlined in section 2.2, L89 is equivalent to equation 7. Guanaco65B can be run with CFG via the CFGLogitsProcessor in cfg.py or via the CFGLM in harness.py.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "7d4e559d-57ab-454c-b418-2a29e7005ff4",
                                    "requirements": "The implementation of CFG which can be used for the Guanaco65B model includes a mutable (not hardcoded) hyper-parameter which can be used to specify the guidance strength",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "Both CFGLM and CFGLogitsProcessor call classifier_free_guidance and all employ a corresponding guidance_scale hparam",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "331299b9-5b4f-45a5-8e7e-8749fdca7871",
                                    "requirements": "As outlined in Section 3.1, the implementation of CFG which can be used for the Guanaco65B model starts the unconditional prompt $\\log p_{\\theta}(w_i | w_{j < i})$ with the last token of the initial prompt.",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "This is implemented in L69-72 of cfg.py",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                }
                            ]
                        },
                        {
                            "id": "db5f1f8c-ec4f-4857-9a78-55d29ba6f063",
                            "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on the WizardLM-30B model.",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "54e5fdc8-4576-4c05-8dd7-3b0e374d6881",
                                    "requirements": "Classifier-Free Guidance (CFG) inference is implemented as described in equation 7 in Section 2.2 such that it can be run on the WizardLM-30B model.",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "The classifier_free_guidance fn in cfg.py implements CFG as outlined in section 2.2, L89 is equivalent to equation 7. WizardLM-30B can be run with CFG via the CFGLogitsProcessor in cfg.py or via the CFGLM in harness.py.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "f5bb6a9b-9599-4bd3-9352-bb673c9be084",
                                    "requirements": "The implementation of CFG which can be used for the WizardLM-30B model includes a mutable (not hardcoded) hyper-parameter which can be used to specify the guidance strength",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "Both CFGLM and CFGLogitsProcessor call classifier_free_guidance and all employ a corresponding guidance_scale hparam",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "3321df8c-c84e-4db7-b3a0-ce784311ef83",
                                    "requirements": "As outlined in Section 3.1, the implementation of CFG which can be used for the WizardLM-30B model starts the unconditional prompt $\\log p_{\\theta}(w_i | w_{j < i})$ with the last token of the initial prompt.",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "This is implemented in L69-72 of cfg.py",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "id": "7f31322c-cb18-4f9a-ab04-c988cbe44ba5",
                    "requirements": "The Guanaco65B and WizardLM-30B models have been evaluated on the arithmetic reasoning tasks (GSM8K and AQuA) with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] (as per Figure 2) using Wang et al. 2023 self-consistency Chain-of-Thought -- performance metrics have been noted.",
                    "weight": 3,
                    "score": 0.4583333333333333,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "f83f4d5c-6c45-4e39-8eaf-e10be5c76ce8",
                            "requirements": "The Guanaco65B and WizardLM-30B models have been evaluated on GSM8k with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] (as per Figure 2) using Wang et al. 2023 self-consistency Chain-of-Thought -- performance metrics have been noted.",
                            "weight": 2,
                            "score": 0.5,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "c400e726-169f-4b44-a8e3-e17ba6e7e30b",
                                    "requirements": "Guanaco65B has been evaluated on GSM8k with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] (as per Figure 2) using Wang et al. 2023 self-consistency Chain-of-Thought -- performance metrics have been noted.",
                                    "weight": 1,
                                    "score": 0.5,
                                    "valid_score": true,
                                    "task_category": null,
                                    "explanation": "Aggregated score from sub-tasks.",
                                    "judge_metadata": null,
                                    "sub_tasks": [
                                        {
                                            "id": "906d86fa-3210-4332-855a-220761268104",
                                            "requirements": "Code has been implemented such that when evaluated, Guanaco65B is prompted with the few-shot prompt for GSM8k from Wang et al. 2023: Self-Consistency Improves Chain of Thought Reasoning in Language Models",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "The harness uses the gsm8k_cot_self_consistency task, whose config file on the eleuther harness points to the desired prompt, so this requirement is satisfied,",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "a35c7964-0519-4cf5-b7db-b6b5af8dac67",
                                            "requirements": "Code has been implemented such that when Guanaco65B is evaluated on GSM8k, the initial prompt is $w_p$, such that only $w_p$ is up-weighted and not $w_{cot}$ and the model's answer $w_{a}$, as described at the end of Section 3.2.",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "The code is setup so that CFG will only condition on the prompt, not the generation which includes CoT, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "9033bacd-94f1-497e-9cd0-8ce0755c30d9",
                                            "requirements": "Code has been implemented such that Guanaco65B can be evaluated on GSM8k with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2]  (as per Figure 2)",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "This is implemented in lines 7-14 of run_3.2.py, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "c3efbff1-2ec3-4d5e-986f-345dfdef0df2",
                                            "requirements": "The evaluation setup has been reproduced, either by directly using the Language Model Evaluation Harness (Gao et al., 2021, github.com/EleutherAI/lm-evaluation-harness) or by reproducing the harness' setup.",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "run_3.2.py uses harness.evaluate, which uses the eleuther LM eval harness, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "0979bbed-aa76-4f3e-b9ed-c9de82f6ab9d",
                                            "requirements": "The evaluation of Guanaco65B on GSM8k with Wang et al. 2023 few-shot prompting has been correctly executed for guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] as outlined in section 3.2",
                                            "weight": 2,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "The evaluation has not been executed, so this requirement is not satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "495dbdd2-9afb-4a38-bc84-8c658ed3bf51",
                                            "requirements": "The evaluation of Guanaco65B on GSM8k, the metric '% of chains ending in a invalid answer' has been correctly measured all guidance strengths.",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "This has not been measured, so this requirement is not satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "7a6728bd-1ca8-4a0b-a297-af9a9f2c9399",
                                            "requirements": "The evaluation of Guanaco65B on GSM8k, the metric 'final answer accuracy' has been correctly measured all guidance strengths.",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "This has not been measured, so this requirement is not satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        }
                                    ]
                                },
                                {
                                    "id": "422e18de-6969-4429-9419-e37a1fca8c32",
                                    "requirements": "WizardLM-30B has been evaluated on GSM8k with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] (as per Figure 2) using Wang et al. 2023 self-consistency Chain-of-Thought -- performance metrics have been noted.",
                                    "weight": 1,
                                    "score": 0.5,
                                    "valid_score": true,
                                    "task_category": null,
                                    "explanation": "Aggregated score from sub-tasks.",
                                    "judge_metadata": null,
                                    "sub_tasks": [
                                        {
                                            "id": "df85c296-ebb2-4622-a441-f194c527c3e8",
                                            "requirements": "Code has been implemented such that when evaluated, WizardLM-30B is prompted with the few-shot prompt for GSM8k from Wang et al. 2023: Self-Consistency Improves Chain of Thought Reasoning in Language Models",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "The harness uses the gsm8k_cot_self_consistency task, whose config file on the eleuther harness points to the desired prompt, so this requirement is satisfied,",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "dd0a21e9-5533-440d-aa93-020c115ff576",
                                            "requirements": "Code has been implemented such that when WizardLM-30B is evaluated on GSM8k, the initial prompt is $w_p$, such that only $w_p$ is up-weighted and not $w_{cot}$ and the model's answer $w_{a}$, as described at the end of Section 3.2.",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "The code is setup so that CFG will only condition on the prompt, not the generation which includes CoT, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "72686517-86bb-40f2-9a79-72ada39736c4",
                                            "requirements": "Code has been implemented such that WizardLM-30B can be evaluated on GSM8k with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2]  (as per Figure 2)",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "This is implemented in lines 7-14 of run_3.2.py, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "84f83715-f610-4974-a827-d7b73a887ebf",
                                            "requirements": "The evaluation setup has been reproduced, either by directly using the Language Model Evaluation Harness (Gao et al., 2021, github.com/EleutherAI/lm-evaluation-harness) or by reproducing the harness' setup.",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "run_3.2.py uses harness.evaluate, which uses the eleuther LM eval harness, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "ed1f6d6e-d5d5-4730-be8b-680a0a7f115f",
                                            "requirements": "The evaluation of WizardLM-30B on GSM8k with Wang et al. 2023 few-shot prompting has been correctly executed for guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] as outlined in section 3.2",
                                            "weight": 2,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "No evaluation of the model has been executed, so this requirement is not satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "61cb3e6e-e29a-400a-8ccd-2eaec6abb5b4",
                                            "requirements": "The evaluation of WizardLM-30B on GSM8k, the metric '% of chains ending in a invalid answer' has been correctly measured all guidance strengths.",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "This has not been measured, so this requirement is not satisfied",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "9e999e7d-a65f-4af6-bd74-73f25705f652",
                                            "requirements": "The evaluation of WizardLM-30B on GSM8k, the metric 'final answer accuracy' has been correctly measured all guidance strengths.",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "This has not been measured, so this requirement is not satisfied",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "id": "7abb2de9-1a6a-4b5b-80e0-f201a552ce30",
                            "requirements": "The Guanaco65B and WizardLM-30B models have been evaluated on AQuA with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] (as per Figure 17) using Wang et al. 2023 self-consistency Chain-of-Thought -- performance metrics have been noted.",
                            "weight": 1,
                            "score": 0.375,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "35d7e329-cc26-43a5-97d7-0b868a369922",
                                    "requirements": "Guanaco65B has been evaluated on AQuA with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] (as per Figure 17) using Wang et al. 2023 self-consistency Chain-of-Thought -- performance metrics have been noted.",
                                    "weight": 1,
                                    "score": 0.375,
                                    "valid_score": true,
                                    "task_category": null,
                                    "explanation": "Aggregated score from sub-tasks.",
                                    "judge_metadata": null,
                                    "sub_tasks": [
                                        {
                                            "id": "36ab3916-12da-4531-ae33-289d41675736",
                                            "requirements": "Code has been implemented such that when evaluated, Guanaco65B is prompted with the few-shot prompt for AQuA from Wang et al. 2023: Self-Consistency Improves Chain of Thought Reasoning in Language Models",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "No few shot prompt is used when inspecting the config file used for AQuA in the eval harness, so this req is not satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "f5bf82f6-8612-4bcf-b50d-5ce25b9bfb9e",
                                            "requirements": "Code has been implemented such that when Guanaco65B is evaluated on AQuA, the initial prompt is $w_p$, such that only $w_p$ is up-weighted and not $w_{cot}$ and the model's answer $w_{a}$, as described at the end of Section 3.2.",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "The code is setup so that CFG will only condition on the prompt, not the generation which includes CoT, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "22878be6-78e2-4c6d-94c0-d8684b2c9a79",
                                            "requirements": "Code has been implemented such that Guanaco65B can be evaluated on AQuA with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2]  (as per Figure 17)",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "This is implemented in lines 7-14 of run_3.2.py, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "334c4c79-57cf-4950-92a2-2a14a3d22b32",
                                            "requirements": "The evaluation setup has been reproduced, either by directly using the Language Model Evaluation Harness (Gao et al., 2021, github.com/EleutherAI/lm-evaluation-harness) or by reproducing the harness' setup.",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "run_3.2.py uses harness.evaluate, which uses the eleuther LM eval harness, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "47090361-273d-4e66-90c2-02516d60d342",
                                            "requirements": "The evaluation of Guanaco65B on AQuA with Wang et al. 2023 few-shot prompting has been correctly executed for guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] as outlined in section 3.2",
                                            "weight": 2,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "No evaluation of the model has been executed, so this requirement is not satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "5c3b3b3c-69e8-4b38-a4df-ea5e51143b87",
                                            "requirements": "The evaluation of Guanaco65B on AQuA, the metric '% of chains ending in a invalid answer' has been correctly measured all guidance strengths.",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "Not been measured, so req not satisfied",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "7dcf053f-2fcb-4bf0-836f-1d5e42bf6c33",
                                            "requirements": "The evaluation of Guanaco65B on AQuA, the metric 'final answer accuracy' has been correctly measured all guidance strengths.",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "Not been measured, so req not satisfied",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        }
                                    ]
                                },
                                {
                                    "id": "2c5204f9-5064-4d61-bb95-933f09f4fb05",
                                    "requirements": "WizardLM-30B has been evaluated on AQuA with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] (as per Figure 17) using Wang et al. 2023 self-consistency Chain-of-Thought -- performance metrics have been noted.",
                                    "weight": 1,
                                    "score": 0.375,
                                    "valid_score": true,
                                    "task_category": null,
                                    "explanation": "Aggregated score from sub-tasks.",
                                    "judge_metadata": null,
                                    "sub_tasks": [
                                        {
                                            "id": "d336a38c-fb1d-47c2-afb3-aa6399a77218",
                                            "requirements": "Code has been implemented such that when evaluated, WizardLM-30B is prompted with the few-shot prompt for AQuA from Wang et al. 2023: Self-Consistency Improves Chain of Thought Reasoning in Language Models",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "No few shot prompt is used when inspecting the config file used for AQuA in the eval harness, so this req is not satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "e8d7ff3a-e7b5-4899-aa29-289e6f7636d1",
                                            "requirements": "Code has been implemented such that when WizardLM-30B is evaluated on AQuA, the initial prompt is $w_p$, such that only $w_p$ is up-weighted and not $w_{cot}$ and the model's answer $w_{a}$, as described at the end of Section 3.2.",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "The code is setup so that CFG will only condition on the prompt, not the generation which includes CoT, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "d8aa828b-d997-43be-92f1-3831005f8e95",
                                            "requirements": "Code has been implemented such that WizardLM-30B can be evaluated on AQuA with guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2]  (as per Figure 17)",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "This is implemented in lines 7-14 of run_3.2.py, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "0da510f9-aaf7-49ee-8557-75f242967fcb",
                                            "requirements": "The evaluation setup has been reproduced, either by directly using the Language Model Evaluation Harness (Gao et al., 2021, github.com/EleutherAI/lm-evaluation-harness) or by reproducing the harness' setup.",
                                            "weight": 1,
                                            "score": 1.0,
                                            "valid_score": true,
                                            "task_category": "Code Development",
                                            "explanation": "run_3.2.py uses harness.evaluate, which uses the eleuther LM eval harness, so this requirement is satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "1a017901-8155-4799-bb68-950df455b21a",
                                            "requirements": "The evaluation of WizardLM-30B on AQuA with Wang et al. 2023 few-shot prompting has been correctly executed for guidance strengths [1, 1.1, 1.25, 1.5, 1.75, 2] as outlined in section 3.2",
                                            "weight": 2,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "No evaluation of the model has been executed, so this requirement is not satisfied.",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "87c32731-ce4a-4a77-9514-7fedda78d8c7",
                                            "requirements": "The evaluation of WizardLM-30B on AQuA, the metric '% of chains ending in a invalid answer' has been correctly measured all guidance strengths.",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "Not been measured, so req not satisfied",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        },
                                        {
                                            "id": "66f11db3-5b1d-48b9-a0a6-7d671f89da93",
                                            "requirements": "The evaluation of WizardLM-30B on AQuA, the metric 'final answer accuracy' has been correctly measured all guidance strengths.",
                                            "weight": 1,
                                            "score": 0.0,
                                            "valid_score": true,
                                            "task_category": "Code Execution",
                                            "explanation": "Not been measured, so req not satisfied",
                                            "judge_metadata": null,
                                            "sub_tasks": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "id": "fdf80c3f-0645-46c5-80b4-3126fe0a1f69",
                    "requirements": "The results of Figure 2 have been replicated.",
                    "weight": 2,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "5bad956d-c7ac-4757-bc35-e48766c376d2",
                            "requirements": "For small values of gamma (less than 1.5), CFG is shown to increase the % of chains ending in a valid answer when tested on the GSM8K dataset, following Figure 2 (Bottom)",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "The experiment has not been run, so no results have been produced and therefore this requirement cannot be verified and is therefore not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "e47a76ae-554f-4a4e-8c77-1ad226a6ebed",
                            "requirements": "For large values of gamma (over 1.5), CFG is shown to maintain a small invalid % of chains ending in a invalid answer while causing a drop in accuracy when tested on the GSM8K dataset, following Figure 2.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "The experiment has not been run, so no results have been produced and therefore this requirement cannot be verified and is therefore not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "217d6a3f-5312-4fa1-b4f5-acf4c1f73504",
                    "requirements": "The results of Figure 17 have been replicated.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "becbf16d-d6f7-4dd3-b06f-fc10b31b6671",
                            "requirements": "For small values of gamma (less than 1.5), CFG is shown to increase the % of chains ending in a valid answer while increasing model accuracy when tested on the AQuA dataset, following Figure 17.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "The experiment has not been run, so no results have been produced and therefore this requirement cannot be verified and is therefore not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "364fe992-0840-4fc8-9e38-28834b43e7ba",
                            "requirements": "For large values of gamma (over 1.5), CFG is shown to maintain a small % of chains ending in a invalid answer while causing a drop in accuracy when tested on the AQuA dataset, following Figure 17.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "The experiment has not been run, so no results have been produced and therefore this requirement cannot be verified and is therefore not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "6f359147-5dd3-43f3-80c5-fbf263dd0e2b",
            "requirements": "CFG is evaluated on HumanEval as per Section 3.3",
            "weight": 4,
            "score": 0.25,
            "valid_score": true,
            "task_category": null,
            "explanation": "Aggregated score from sub-tasks.",
            "judge_metadata": null,
            "sub_tasks": [
                {
                    "id": "d671a00f-a2d4-447c-95e0-78fef6e83c4f",
                    "requirements": "Code for acquiring the the HumanEval dataset (https://github.com/openai/human-eval) from its official source or equivalent mirrors has been implemented and can be used",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": "Code Development",
                    "explanation": "There is no code that does this, so this requirement is not satisified.",
                    "judge_metadata": null,
                    "sub_tasks": []
                },
                {
                    "id": "1e8916bc-5a46-4d46-a3a7-aae9d9c8cb67",
                    "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on the CodeGen family of models (CodeGen-350M-mono, CodeGen-2B-mono, CodeGen-6B-mono)",
                    "weight": 2,
                    "score": 1.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "6d62cf3f-463e-42f4-a9b3-1896b52325cc",
                            "requirements": "Classifier-Free Guidance (CFG) inference is implemented as described in equation 7 in Section 2.2 such that it can be run on the CodeGen family of models (CodeGen-350M-mono, CodeGen-2B-mono, CodeGen-6B-mono)",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "The classifier_free_guidance fn in cfg.py implements CFG as outlined in section 2.2, L89 is equivalent to equation 7. CodeGen models can be run with CFG via the CFGLogitsProcessor in cfg.py or via the CFGLM in harness.py.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "0c8c0598-a5c0-4484-8d16-b98671646528",
                            "requirements": "The implementation of CFG which can be used for the CodeGen family of models (CodeGen-350M-mono, CodeGen-2B-mono, CodeGen-6B-mono) includes a mutable (not hardcoded) hyper-parameter which can be used to specify the guidance strength",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "Both CFGLM and CFGLogitsProcessor call classifier_free_guidance and all employ a corresponding guidance_scale hparam",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "ff66683d-f2cf-4d57-b235-d028dfc1711c",
                            "requirements": "As outlined in Section 3.1, the implementation of CFG which can be used for the CodeGen family of models (CodeGen-350M-mono, CodeGen-2B-mono, CodeGen-6B-mono) starts the unconditional prompt $\\log p_{\\theta}(w_i | w_{j < i})$ with the last token of the initial prompt.",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This is implemented in L69-72 of cfg.py",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "08a827c7-b943-44b1-91da-e682f4a0e8a6",
                    "requirements": "CodeGen variants (350M-mono, 2B-mono, 6B-mono) have been evaluated on HumanEval using various gamma values at [1.0, 1.1, 1.25, 1.5, 1.75, 2.0] and various sampling temperatures at [0.2, 0.6, 0.8]. pass@k (following Chen et al., 2021, see footnote 4) for k = 1, 10, 100 is collected.",
                    "weight": 3,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "8b8c4106-59a9-47f3-bfaf-3390f9b25c4f",
                            "requirements": "Code has been implemented such that the CodeGen model family can be evaluated on HumanEval with guidance strengths [1.0, 1.1, 1.25, 1.5, 1.75, 2.0] as outlined in Footnote 3",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "There is no code that does this, so this requirement is not satisified.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "78e019d5-21f7-4358-b5e8-1c898dbfa93a",
                            "requirements": "Code has been implemented such that the CodeGen model family can be evaluated on HumanEval with a sampling temperatures of [0.2, 0.6, 0.8] as outlined in Figure 3.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "There is no code that does this, so this requirement is not satisified.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "fdddda96-3a26-4234-b00f-dccd5cadc763",
                            "requirements": "Code has been implemented such that the CodeGen model family performance on HumanEval can measured with pass@k, for k = 1, 10, 100 as outlined in Section 3.3.1 and in Footnote 4.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "There is no code that does this, so this requirement is not satisified.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "cdf2074b-47ad-4299-93f1-52783c9b4a59",
                            "requirements": "Code for measuring the number of wins, ties and losses across HumanEval Samples between CFG (\\gamma=1.25) and no CFG (\\gamma=1) has been implemented for CodeGen-350M-mono for temperatures 0.2, 0.6, 0.8 has been implemented. A win is when CFG answers a sample correctly while no CFG does not, a loss is the reciprocal case, and a tie is when CFG and no CFG both get the sample correct or both get the sample incorrect.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "There is no code that does this, so this requirement is not satisified.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "ea86c645-1caa-4cd8-b620-384107b00260",
                            "requirements": "The CodeGen model family(CodeGen-350M-mono, CodeGen-2B-mono, CodeGen-6B-mono) has been evaluted on HumanEval at guidance strengths of  [1.0, 1.1, 1.25, 1.5, 1.75, 2.0], and various sampling temperatures at [0.2, 0.6, 0.8]. The pass@k performance has been measured for k=1,10, 100.  As outlined in Section 3.3.",
                            "weight": 4,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "Nothing from this experiment has been run, so this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "c400edbe-87d4-45dd-b112-ee465ff2d020",
                    "requirements": "The results of Table 2 have been reproduced",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "828ecf74-0518-4bb0-8f5d-c174f6ce8c0c",
                            "requirements": "Across guidance strengths [1.0, 1.1, 1.25, 1.5, 1.75, 2.0], performance metrics measured at a sampling temperature of 0.2 for models 350M-mono, 2B-mono, and 6B-mono, across k-values of 1, 10, and 100, show that using a guidance strength greater than 1.0 (CFG) generally leads to better performance compared to using a guidance strength of 1.0 (no CFG). However, the optimal guidance strength (gamma) varies depending on the specific combination of model size and k-value, as demonstrated in Table 2 of the paper.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "The experiment has not been run, so no results have been produced and therefore this requirement cannot be verified and is therefore not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "1887f8a6-816e-451d-b640-f2f30b9db85e",
                            "requirements": "Across guidance strengths [1.0, 1.1, 1.25, 1.5, 1.75, 2.0], performance metrics measured at a sampling temperature of 0.2 for models 350M-mono, 2B-mono, and 6B-mono,  show that the pass@1 rate increases with CFG across $1 \\leq \\gamma \\leq 1.5$ and degrades thereafter",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "The experiment has not been run, so no results have been produced and therefore this requirement cannot be verified and is therefore not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "13058151-a818-4987-b6e3-313821d63a97",
                            "requirements": "Across guidance strengths [1.0, 1.1, 1.25, 1.5, 1.75, 2.0], performance metrics measured at a sampling temperature of 0.2 for models 350M-mono, 2B-mono, and 6B-mono, across k-values of 1, 10, and 100, show that high k values (pass@100), CFG hurts performance, with guidance strength = 1 (no CFG) outperforming all other guidance strengths for each model at pass@100. As pointed out in Section 3.3.1",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "The experiment has not been run, so no results have been produced and therefore this requirement cannot be verified and is therefore not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "3beb3495-e001-4c6d-9a53-f9c253f6490d",
                    "requirements": "In terms of win rate, for CodeGen-350M-mono, CFG with gamma = 1.25 is found to outperform CFG with gamma=1 on HumanEval tasks at pass@1, across all temperatures 0.2, 0.6 and 0.8, roughly matching the results in Figure 3 (NOT Table 3 as written in the paper - this is an error)",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": "Result Analysis",
                    "explanation": "The experiment has not been run, so no results have been produced and therefore this requirement cannot be verified and is therefore not satisfied",
                    "judge_metadata": null,
                    "sub_tasks": []
                }
            ]
        },
        {
            "id": "03a482fb-d0b3-4e0e-84ba-f7496d3af560",
            "requirements": "The results from the FLOPs analysis of Section 4 have been reproduced, demonstrating performance equivalence between models using CFG and models of double the parameter count.",
            "weight": 1,
            "score": 0.0,
            "valid_score": true,
            "task_category": null,
            "explanation": "Aggregated score from sub-tasks.",
            "judge_metadata": null,
            "sub_tasks": [
                {
                    "id": "0bd0732b-a231-409f-a609-859ea2c1d7d7",
                    "requirements": "Code has been implemented for determining the inference FLOP count for each of the models successfully evaluated in 3.1 as outlined in https://github.com/google-research/electra/blob/master/flops_computation.py, with and without CFG.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": "Code Development",
                    "explanation": "This has not been implemented, so this requirement is not satisfied",
                    "judge_metadata": null,
                    "sub_tasks": []
                },
                {
                    "id": "3a1e0124-f2bc-4bb9-8fbe-cbafa2b39ccc",
                    "requirements": "Code has been implemented for pairing the inference FLOP with the model's evaluation's results from 3.1.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": "Code Development",
                    "explanation": "This has not been implemented, so this requirement is not satisfied",
                    "judge_metadata": null,
                    "sub_tasks": []
                },
                {
                    "id": "6dff4cf6-8d6a-4e9d-81c0-bd5c170a918e",
                    "requirements": "The inference FLOP count for each of the models successfully evaluated in 3.1 with and without CFG has been computed as outlined in https://github.com/google-research/electra/blob/master/flops_computation.py and paired with the evaluation results from 3.1.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": "Code Execution",
                    "explanation": "This has not been implemented nor computed, so this requirement is not satisfied",
                    "judge_metadata": null,
                    "sub_tasks": []
                },
                {
                    "id": "65962a7d-23b7-49e8-9fc6-9ef8601081eb",
                    "requirements": "The results show that across 5 out of 9 tasks there is a statistically insignificant difference between using CFG and vanilla prompting with a model of twice the size has been replicated using ANCOVA regression analysis, following the procedure outlined in Appendix C.2. It is found that of the significantly different tasks, 2 favor CFG and 2 favor vanilla.",
                    "weight": 2,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "acc961fa-f5b9-4bd9-8649-aba832a6ed20",
                            "requirements": "Code is developed to perform ANCOVA regression analysis on the performance data from Section 3.1 as described in Appendix C.2.",
                            "weight": 2,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This has not been implemented, so this requirement is not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "0c6856a6-6f59-40f0-9b49-80abfec0bf08",
                            "requirements": "The ANCOVA regression analysis has been run on the performance data of at least 5 of the 9 tasks from Section 3.1 and results for each task have been recorded.",
                            "weight": 2,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "This has not been run nor implemented, so this requirement is not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "0bf71c77-8ce3-4878-b277-8856d10ebc26",
                            "requirements": "The ANCOVA results show that across the majority of the tasks, statistically in-significant difference between using CFG and using vanilla prompting with a model of twice the size at p=0.01, while for the remaining tasks, there is an even amount of tasks that favour CFG and tasks that favour 2x vanilla. These results roughly match what is shown in Table 6 (NOT Table 9 as stated in the paper - this is an error).",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "This experiment has not been run, so there are no results to check, and therefore this requirement cannot be satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "0f057b09-5df1-4955-98c8-65d2176c0065",
            "requirements": "The results of Section 5 have been reproduced",
            "weight": 1,
            "score": 0.2857142857142857,
            "valid_score": true,
            "task_category": null,
            "explanation": "Aggregated score from sub-tasks.",
            "judge_metadata": null,
            "sub_tasks": [
                {
                    "id": "7c686377-f0e3-46aa-a305-cabe1856ff559",
                    "requirements": "Code for acquiring around 32k samples from the P3 dataset (https://huggingface.co/datasets/bigscience/P3) from its official source or equivalent mirrors has been implemented and can be used, as outlined in Section 5.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": "Code Development",
                    "explanation": "There is no code that does this, so this requirement is not satisified.",
                    "judge_metadata": null,
                    "sub_tasks": []
                },
                {
                    "id": "de262f1f-8247-4b20-85b5-94baeaffa90d",
                    "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on Falcon-7b Base and Instruct models",
                    "weight": 2,
                    "score": 1.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "3bfd72f3-5535-4e4b-b22a-12c7ef487b38",
                            "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on the Falcon-7b-Base model",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "743ce2d4-c6a1-46dc-a69d-36c172f5c3a3",
                                    "requirements": "Classifier-Free Guidance (CFG) inference is implemented as described in equation 7 in Section 2.2 such that it can be run on the Falcon-7b-Base model",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "The classifier_free_guidance fn in cfg.py implements CFG as outlined in section 2.2, L89 is equivalent to equation 7. Falcon-7b-Base can be run with CFG via the CFGLogitsProcessor in cfg.py or via the CFGLM in harness.py.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "d6ae86d0-856c-403c-993b-91f87ad53f09",
                                    "requirements": "The implementation of CFG which can be used for the Falcon-7b-Base model of models includes a mutable (not hardcoded) hyper-parameter which can be used to specify the guidance strength",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "Both CFGLM and CFGLogitsProcessor call classifier_free_guidance and all employ a corresponding guidance_scale hparam",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "fa75e956-d82d-4148-95fd-7c034246100d",
                                    "requirements": "As outlined in Section 3.1, the implementation of CFG which can be used for the Falcon-7b-Base model starts the unconditional prompt $\\log p_{\\theta}(w_i | w_{j < i})$ with the last token of the initial prompt for the Falcon-7b-Base model",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "This is implemented in L69-72 of cfg.py",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                }
                            ]
                        },
                        {
                            "id": "5043b03e-11df-4536-b28f-a7f6914b064f",
                            "requirements": "Classifier-Free Guidance inference is implemented as described in Equation 7 in Section 2.2 and in Section 3.1 such that it can be run on the Falcon-7b-Instruct model",
                            "weight": 1,
                            "score": 1.0,
                            "valid_score": true,
                            "task_category": null,
                            "explanation": "Aggregated score from sub-tasks.",
                            "judge_metadata": null,
                            "sub_tasks": [
                                {
                                    "id": "b28bee93-169d-444a-b03c-09a6e183e3c5",
                                    "requirements": "Classifier-Free Guidance (CFG) inference is implemented as described in equation 7 in Section 2.2 such that it can be run on the Falcon-7b-Instruct model",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "The classifier_free_guidance fn in cfg.py implements CFG as outlined in section 2.2, L89 is equivalent to equation 7. Falcon-7b-Instruct can be run with CFG via the CFGLogitsProcessor in cfg.py or via the CFGLM in harness.py.",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "84f2d615-9843-48af-9c81-a85ff84cb6e5",
                                    "requirements": "The implementation of CFG which can be used for the Falcon-7b-Instruct model of models includes a mutable (not hardcoded) hyper-parameter which can be used to specify the guidance strength",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "Both CFGLM and CFGLogitsProcessor call classifier_free_guidance and all employ a corresponding guidance_scale hparam",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                },
                                {
                                    "id": "ad6259e5-4211-4344-97c3-5ba0d2145976",
                                    "requirements": "As outlined in Section 3.1, the implementation of CFG which can be used for the Falcon-7b-Instruct model starts the unconditional prompt $\\log p_{\\theta}(w_i | w_{j < i})$ with the last token of the initial prompt for the Falcon-7b-Instruct model",
                                    "weight": 1,
                                    "score": 1.0,
                                    "valid_score": true,
                                    "task_category": "Code Development",
                                    "explanation": "This is implemented in L69-72 of cfg.py",
                                    "judge_metadata": null,
                                    "sub_tasks": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "id": "03721f72-87d3-4e1a-a475-5e5c2269a4ea",
                    "requirements": "Code for measuring the average logit entropy has been implemented, where entropy is defined $H(p) = -\\sum_k p_k \\log p_k$, equivalent to the scipy implementation as outlined in the Addendum.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": "Code Development",
                    "explanation": "There is no code that does this, so this requirement is not satisified.",
                    "judge_metadata": null,
                    "sub_tasks": []
                },
                {
                    "id": "4c213fb4-197a-4483-b25a-f832c73f2659",
                    "requirements": "The results of Section 5.1, studying CFG's impact on logit entropy, have been reproduced.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "355ba764-e138-4354-867a-dd462854f004",
                            "requirements": "Falcon-7b-Base has been run with (\\gamma=1.5) and without (\\gamma=1) CFG on P3 while measuring the average logit entropy over the sequence",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "The model has not been run on anything, so this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "0b370f02-654c-444a-90e7-ce47d8318b42",
                            "requirements": "The measured logit entropies shows that the CFG inference (\\gamma=1.5) entropy distribution is lower on average vanilla inference (\\gamma=1) entropy distribution, as pointed out in Section 5.1 and shown in Figure 18a.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "Nothing has been run or measured or implemented to this end, so this requirement is not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "b5dfc935-b348-4313-93b2-ecacd6148e48",
                    "requirements": "The results of section 5.2, studying CFG's relation to Instruction Tuning have been reproduced",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "c11a9b64-38f6-4915-9f89-7c62e609e29f",
                            "requirements": "Falcon-7b-Base has been run with (\\gamma=1.5) and Falcon-7b-instruct has been run without (\\gamma=1) CFG on P3 while measuring the average logit entropy over the sequence.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "The model has not been run on anything, so this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "4c213fgh-197a-49993-b25a-f832fg3f2659",
                            "requirements": "The measured logit entropies show that CFG and instruction-tuned models produce similar entropy across generation samples, as shown in Figure 18a.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "Nothing has been run or measured or implemented to this end, so this requirement is not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "3ba84eb6-dc62-47ba-94c8-de5d99539dd7",
                            "requirements": "Code has been implemented for measuring the top-p overlap for some p, where two top-p vocab distributions are input and their inner product is taken to calculate the overlap, as shown in Figure 18b.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This has not been implemented, so this requirement has not been satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "d4ff56a5-2c36-449e-9c9c-f04cc7406767",
                            "requirements": "Falcon-7b-Base has been run with (\\gamma=1.5) and Falcon-7b-instruct has been run without (\\gamma=1) CFG on P3 while measuring top-p overlap between the two at top-p=90%.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "Nothing has been run or measured or implemented so this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "4e501537-9c5b-405d-bf50-8e2e2555a056",
                            "requirements": "The results show that CFG and instruction-tuned models have vocabulary distributions that are largely not overlapping, with a top-p overlap of less than 0.3 as outlined in section 5.2 and highlighted in Figure 18b.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "This experiment has not been run, so there are no results to check, and therefore this requirement cannot be satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "dbfc55c1-253e-4a83-9933-4225f713cc69",
                            "requirements": "Code has been implemented for measuring the perplexity of completion tokens compatible with the CFG and instruction-tuned models.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This has not been implemented, so this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "11e5f977-ec10-40b7-bcdf-36396eebfc97",
                            "requirements": "Falcon-7b-Base has been run with (\\gamma=1.5) and Falcon-7b-instruct has been run without (\\gamma=1) CFG on P3 while measuring the generation perplexity of the two models.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "Neither of the models have been run on anything, so this requirement is not satisified.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "b4b221de-55ca-4bbe-86ba-a84e4a9e2ad5",
                            "requirements": "Code has been implemented for measuring the spearman correlation between two sets of measured perplexities.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This has not been implemented, so this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "884f1af6-3d92-4a29-ac27-e7613dae8b10",
                            "requirements": "Significant spearman correlations ($r_{s}>.7$) between the perplexities of Instruction-Tuned models and CFG models are observed for longer prompts, as shown in Figure 5 (NOT Table 5 as stated in the paper - this is an error) and pointed out in Section 5.2.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "These have not been measured, because nothing was run to this end, so this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "5140765f-8e39-4b89-886e-cc238b2f7a25",
                    "requirements": "Table 3 is replicated using the method described in Section 5.3, demonstrating that CFG encourages tokens about flying dragons and Paris while discourages other topics or regions.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "b676dea1-b5c0-4bc5-a04f-8dd29b3ee8a3",
                            "requirements": "Code to calculate the differences in token log probabilities across the entire vocabulary with CFG at each generation step has been developed as described in Section 5.3, i.e. $P(w_t \\mid w_{<t}) - \\log P(w_T \\mid \\hat{w})$",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This has not been implemented, so this requirement is not satisfied",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "0298b200-a4f2-44f4-80ed-d86191f46609",
                            "requirements": "Falcon-7B-base has been sampled with CFG (\\gamma > 1) on the prompt 'The dragon flew over Paris, France', and no negative prompt, measuring the differences log probabilities across the entire vocabulary at each generation step.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "The Model has not been sampled, so this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "d9cf875e-720d-46a1-92ca-c4f4fb2979ea",
                            "requirements": "The results show that CFG encourages token related to 'dragon' and 'Paris' and discourages other topics and regions, by noting more tokens related to the former in the top5 at each time step, and more tokens related to the latter in the bottom5 at each time step. As shown in Table 3.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Result Analysis",
                            "explanation": "The experiment has not been run, so there are no results to check, and therefore this requirement is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "75b989e7-e3dc-42f6-acea-5d88051d0e6b",
            "requirements": "The results from Table 4 in Section 6 have been reproduced",
            "weight": 1,
            "score": 0.0,
            "valid_score": true,
            "task_category": null,
            "explanation": "Aggregated score from sub-tasks.",
            "judge_metadata": null,
            "sub_tasks": [
                {
                    "id": "f3a5c3a1-9c3d-42f9-b831-4a6c5d948e1d",
                    "requirements": "The toxicity detection experiment has been run",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "9d5cdecf-8f7d-4df3-a58f-83c6ae6ed7bd",
                            "requirements": "Code for acquiring the toxicity dataset (https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This has not been implemented, so this requirement is not satisified",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "784f3d0a-d203-4d73-9735-c796e9d01640",
                            "requirements": "The pretrained classifiers used for toxicity evaluation and guidance ('unitary/toxic-bert') has been acquired and set up as described in footnote 10 and Section 6.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "These have not been acquired, so this req is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "5a8d113c-8881-4ea5-82a7-9cf47af47ae9",
                            "requirements": "FUDGE has been set up following Yang & Klein 2021 for GPT-2 to enable classifier guidance on the task of toxicity detection",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "FUDGE has not been setup or implemented at all, so this req is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "57ea7dcb-9922-477e-90c8-0a4a174dbd3c",
                            "requirements": "GPT-2 has been prompted with \"Don't be mean\" for toxicity using CFG and FUDGE, and outputs have been generated and stored.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "There are no traces of this prompt occurring, so this req is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "034ce94b-90ab-4f20-a1f6-244712fb3a44",
                            "requirements": "The generated outputs for toxicity have been evaluated using 'unitary/toxic-bert' to calculate the classification likelihood increase for each approach (FUDGE, CFG).",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "This has not been run, so this req has not been satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "a73d22b4-bf6b-44e9-9c28-37cde7d3bbae",
                    "requirements": "The sentiment analysis experiment has been run",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": null,
                    "explanation": "Aggregated score from sub-tasks.",
                    "judge_metadata": null,
                    "sub_tasks": [
                        {
                            "id": "64bab617-cd05-4c32-b61b-7ac5067d6292",
                            "requirements": "Code for acquiring the sentiment IMBD dataset (https://huggingface.co/datasets/stanfordnlp/imdb) from its official source or equivalent mirrors has been implemented and can be used",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This has not been implemented, so this req is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "e19fd65f-5c5b-4ec4-8c18-3c5e21d54d4a",
                            "requirements": "The pretrained classifier used for sentiment guidance ('bhadresh-savani/distilbert-base-uncased-emotion') has been acquired and set up as described in footnote 10 and Section 6.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This requirement is not satisfied, no pretrained sentiment guidance classifier has been acquired.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "e0e79e7d-96fe-4c8e-b160-2fe1d1c376b8",
                            "requirements": "The pretrained classifier used for sentiment evaluation ('stevhliu/my_awesome_model') has been acquired and set up as described in footnote 10 and Section 6.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "This requirement is not satisfied, no pretrained sentiment guidance evaluation model has been acquired.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "d3ebdc42-6c14-45f8-b49e-8b8f5b460fed",
                            "requirements": "FUDGE has been set up following Yang & Klein 2021 for GPT-2 to enable classifier guidance on the task of sentiment detection",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Development",
                            "explanation": "FUDGE has not been implemented, there are no traces, so this req is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "ad38173f-cbd4-4f45-9cb3-4f7e56b8f041",
                            "requirements": "GPT-2 has been prompted with 'That was a good movie!' for sentiment using CFG and FUDGE, and outputs have been generated and stored.",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "The model was never prompted with that from what can be observed, so the req is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        },
                        {
                            "id": "a1fdc3a9-0c74-4ff7-8c38-e23c120dbde2",
                            "requirements": "The generated outputs for sentiment have been evaluated using 'stevhliu/my_awesome_model' to calculate the classification likelihood increase for each approach (FUDGE, CFG).",
                            "weight": 1,
                            "score": 0.0,
                            "valid_score": true,
                            "task_category": "Code Execution",
                            "explanation": "Nothing has been evaluated to this end, thus this req is not satisfied.",
                            "judge_metadata": null,
                            "sub_tasks": []
                        }
                    ]
                },
                {
                    "id": "225debd0-3ce3-46a0-9729-79bef3467b27",
                    "requirements": "As a result of running the toxicity and/or sentiment analysis experiment(s), comparing the difference between measured % increased in classification likelihood with FUDGE and CFG shows that CFG is able to steer guidance to a much greater degree, with the % increase being higher when using CFG than when using FUDGE for guidance, as shown in Table 4.",
                    "weight": 1,
                    "score": 0.0,
                    "valid_score": true,
                    "task_category": "Result Analysis",
                    "explanation": "The experiment has not been run at all, so there are not results to check for this statement, so this requirement is not satisfied.",
                    "judge_metadata": null,
                    "sub_tasks": []
                }
            ]
        }
    ]
}
